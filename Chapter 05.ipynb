{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confidential-border",
   "metadata": {},
   "source": [
    "# Abstract of airflow chapter 05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-indonesia",
   "metadata": {},
   "source": [
    "### Defining dependencies between tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-microphone",
   "metadata": {},
   "source": [
    "In this chapter the idea is to focus on dependencies (linear and multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-governor",
   "metadata": {},
   "source": [
    "### Linear dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-sheffield",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH_02_rocket_download_pipeline.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-questionnaire",
   "metadata": {},
   "source": [
    "In this type of DAG, each task must be completed before going on to the next, as the result of the preceding task is required as an input for the next. It could be done usign the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-encounter",
   "metadata": {},
   "source": [
    "```python\n",
    "# Set task dependencies one-by-one: download_launches >> get_pictures get_pictures >> notify\n",
    "# Or in one go:\n",
    "download_launches >> get_pictures >> notify\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-wrong",
   "metadata": {},
   "source": [
    "Task dependencies effectively tell Airflow that it can only start executing a given task once its upstream dependencies have finished executing successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-saudi",
   "metadata": {},
   "source": [
    "### Fan-in/-out Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-oliver",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH05_final_dag.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-publisher",
   "metadata": {},
   "source": [
    "```python\n",
    "[clean_weather, clean_sales] >> join_datasets\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-desktop",
   "metadata": {},
   "source": [
    "fan in dependency, add a Dummy operator to be the started point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-hypothesis",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "# Create a dummy start task.\n",
    "start = DummyOperator(task_id=\"start\")\n",
    "# Fan-out (one-to-multiple).\n",
    "start >> [fetch_weather, fetch_sales]\n",
    "[clean_weather, clean_sales] >> join_datasets\n",
    "join_datasets >> train_model >> deploy_model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-approach",
   "metadata": {},
   "source": [
    "The execution order will be 1, 2a, 2b, 3a, 3b(in paralel), 4, 5, 6. Supose your company changes the ERP system, you must adapt your workflow, to do that a posible solution could be add a new task to get the data and split by time when the new system will start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-pepper",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH05 taskbranhc.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-programmer",
   "metadata": {},
   "source": [
    "The task is easy we only need to create new flows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-mobile",
   "metadata": {},
   "source": [
    "```python\n",
    "fetch_sales_old >> clean_sales_old\n",
    "fetch_sales_new >> clean_sales_new\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-archive",
   "metadata": {},
   "source": [
    "Now we still need to connect these tasks to the rest of our DAG and make sure that Airflow knows which of these tasks it should execute when. Fortunately, Airflow provides built-in support for choosing between sets of downstream tasks using the BranchPythonOperator. However, in contrast to the PythonOperator, callables passed to the BranchPythonOperator are expected to return the ID of a downstream task as a result of their computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-tenant",
   "metadata": {},
   "source": [
    "```python\n",
    "def _pick_erp_system(**context)\n",
    "\n",
    "pick_erp_system = BranchPythonOperator( task_id='pick_erp_system',\n",
    " python_callable=_pick_erp_system, \n",
    ")\n",
    "\n",
    "def _pick_erp_system(**context):\n",
    "    if context[\"execution_date\"] < ERP_SWITCH_DATE:\n",
    "        return \"fetch_sales_old\" \n",
    "    else:\n",
    "        return \"fetch_sales_new\"\n",
    "\n",
    "pick_erp_system = BranchPythonOperator(task_id='pick_erp_system',\n",
    "                                       python_callable=_pick_erp_system)\n",
    "                                       \n",
    "pick_erp_system >> [fetch_sales_old, fetch_sales_new]\n",
    "start_task >> pick_erp_system\n",
    "[clean_sales_old, clean_sales_new] >> join_datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-replica",
   "metadata": {},
   "source": [
    "However, if you do this, running the DAG would result in the join_datasets task and all its downstream tasks being skipped by Airflow. (You can try it out if you wish!)\n",
    "The reason for this is that, by default, Airflow requires all tasks upstream of a given task to complete successfully before that the task itself can be executed. By connecting both of our cleaning tasks to the join_datasets task, we created a situation where this can never occur, as only one of the cleaning tasks is ever executed! As a result, the join_datasets task can never be executed and is skipped by Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-secondary",
   "metadata": {},
   "source": [
    "This behavior that defines when tasks are executed is controlled by so-called ‘trigger rules’ in Airflow. Trigger rules can be defined for individual tasks using the trigger_rule argument, which can be passed to any operator. By default, trigger rules are set to ‘all_success’, meaning that all parents of the corresponding task need to succeed before the task can be run. This never happens when using the BranchPythonOperator, as it skips any tasks that are not chosen by the branch, which explains why the join_datasets task and all its downstream tasks were also skipped by Airflow.\n",
    "To fix this situation, we can change the trigger rule of join_datasets so that it can still trigger if one of its upstream tasks is skipped. One way to achieve this is to change the trigger rule to `none_failed`, which specifies that a task should run as soon as all of its parents are done with executing and none have failed:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-portuguese",
   "metadata": {},
   "source": [
    "```python\n",
    "join_datasets = PythonOperator(trigger_rule=\"none_failed\", )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-tucson",
   "metadata": {},
   "source": [
    "This way, join_datasets will start executing as soon as all of its parents have finished executing without any failures, allowing join_datasets to continue its execution after the branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-soundtrack",
   "metadata": {},
   "source": [
    "One drawback of this approach is that we now have three edges going into the join_datasets task. This doesn’t really reflect the nature of our flow, in which we essentially want to fetch sales/weather data (choosing between the two ERP systems first) and then feed these two data sources into join_datasets. For this reason, many people choose to make the branch condition more explicit by adding a dummy task joining the different branches before continuing with the DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-gravity",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "\n",
    "join_branch = DummyOperator(task_id=\"join_erp_branch\", trigger_rule=\"none_failed\")\n",
    "\n",
    "[clean_sales_old, clean_sales_new] >> join_branch\n",
    "join_branch >> join_datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-washington",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH05 3edge.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-college",
   "metadata": {},
   "source": [
    "This change also means that we no longer need to change the trigger rule for the join_datasets task, making our branch more self-contained than the original. Another problem is the deploy, it will run for every execution not only for the last, to solve that the correct approach should be create a new task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "commercial-thesis",
   "metadata": {},
   "source": [
    "### 5.4 More about Trigger Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-foster",
   "metadata": {},
   "source": [
    "\n",
    "In the previous sections, we have seen how Airflow allows us to build dynamic behavior DAGs, which allows us to encode branches or conditional statements directly into our DAGs. Much of this behavior is governed by Airflow’s so-called trigger rules, which determine exactly when a task is executed by Airflow. As we skipped over trigger rules relatively quickly in the previous sections, we’ll explore them in a bit more detail here to give you a feeling of what trigger rules represent and what you can do with them.\n",
    "To understand trigger rules, we first have to examine how Airflow executes tasks within a DAG run. In essence, when Airflow is executing a DAG, it continuously checks each of your tasks to see whether it can be executed. As soon as a task is deemed ‘ready for execution’, the task is picked up by the scheduler and scheduled to be executed. As a result, the task is executed as soon as Airflow has an execution slot available.\n",
    "So how does Airflow determine when a task can be executed? That is where trigger rules come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-flood",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH05 triggers01.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-customer",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH05 triggers02.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-lemon",
   "metadata": {},
   "source": [
    "### Using xcom to share state between tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-requirement",
   "metadata": {},
   "source": [
    "Dont use xcom!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-dragon",
   "metadata": {},
   "source": [
    "### Using API taskflow to share state between tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-morris",
   "metadata": {},
   "source": [
    "Dont use taskflow its only a decorator for XCOM!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-outside",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-toolbox",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-senegal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-throat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-begin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-spending",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
