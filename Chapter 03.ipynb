{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "popular-produce",
   "metadata": {},
   "source": [
    "# Abstract of airflow chapter 03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-spiritual",
   "metadata": {},
   "source": [
    "### Scheduling in Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-abraham",
   "metadata": {},
   "source": [
    "This chapter goes deeper in the concept of how to create a dag in airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-chapel",
   "metadata": {},
   "source": [
    "``` python\n",
    "dag = DAG(\n",
    "dag_id=\"03_with_end_date\", \n",
    "    schedule_interval=\"@daily\", \n",
    "    start_date=dt.datetime(year=2019, month=1, day=1), \n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-choice",
   "metadata": {},
   "source": [
    "The start and end day begins at midnight. To support more complicated scheduling intervals, Airflow allows us to define scheduling intervals using the same syntax as used by cron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-complement",
   "metadata": {},
   "source": [
    "An important limitation of cron expressions is that they are unable to represent certain frequency- based schedules, ariflow can handle that using timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-realtor",
   "metadata": {},
   "source": [
    "```python\n",
    "dag = DAG(\n",
    "    dag_id=\"03_with_end_date\",\n",
    "    schedule_interval=timedelta(days=3),\n",
    "    start_date=dt.datetime(year=2019, month=1, day=1), \n",
    "    end_date=dt.datetime(year=2019, month=1, day=5),\n",
    "    ) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-proposal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "professional-resort",
   "metadata": {},
   "source": [
    "For one, our DAG is downloading and calculating statistics for the entire catalog of user events every day, which is hardly efficient. Moreover, this process is only downloading events for the past 30 days, which means that we are not building up any history for dates further in the past.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-regular",
   "metadata": {},
   "source": [
    "Airflow con handle that with Fetching events incrementally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-grave",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "fetch_events = BashOperator(task_id=\"fetch_events\", \n",
    "bash_command=(\n",
    "\"mkdir -p /data && \"\n",
    "\"curl -o /data/events.json \"\n",
    " \"http://localhost:5000/events?\" \"start_date=2019-01-01&\" \"end_date=2019-01-02\"\n",
    "),\n",
    "dag=dag)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-apache",
   "metadata": {},
   "source": [
    "The problem of the above approach is to use other dates than 2019-01-01 and 2019-01-02 for that we need aiflow feature: Dynamic time references using execution dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-navigator",
   "metadata": {},
   "source": [
    "####  Dynamic time references using execution dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-entity",
   "metadata": {},
   "source": [
    "For many workflows involving time-based processes, it is important to know for which time interval a given task is being executed. For this reason, Airflow provides tasks with extra parameters that can be used to determine for which schedule interval a task is being executed (we’ll go into more detail on these ‘parameters’ in the next chapter).\n",
    "The most important of these parameters is called the execution_date, which represents the date and time for which our DAG is being executed. In contrast to what the name of the parameter suggests, the execution_date is not a date but a timestamp, which reflects the start time of the schedule interval for which the DAG is being executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "digital-neighbor",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH03extra_params.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-wayne",
   "metadata": {},
   "source": [
    "In Airflow, we can use these execution dates by referencing them in our operators. For example, in the BashOperator, we can use Airflow’s templating functionality to include the execution dates dynamically in our Bash command. Templating is covered in detail in Chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-medium",
   "metadata": {},
   "source": [
    "Using airflow extra parameters we can use the execution date:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-watershed",
   "metadata": {},
   "source": [
    "```python\n",
    "fetch_events = BashOperator( task_id=\"fetch_events\", \n",
    "bash_command=(\n",
    "\"mkdir -p /data && \"\n",
    "\"curl -o /data/events.json \"\n",
    "\"http://localhost:5000/events?\" \"start_date={{execution_date.strftime('%Y-%m-%d')}}\" #A \"&end_date={{next_execution_date.strftime('%Y-%m-%d')}}\" #B\n",
    "),\n",
    "dag=dag, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-milwaukee",
   "metadata": {},
   "source": [
    "In this example, the syntax {{variable_name}} is an example of using Airflow’s Jinja-based templating syntax for referencing one of Airflow’s specific parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-comfort",
   "metadata": {},
   "source": [
    "For example, ds and ds_nodash parameters are different representations of the execution_date, formatted as YYYY-MM-DD and YYYYMMDD respectively. Similarly, the next_ds, next_ds_nodash, prev_ds, and prev_ds_nodash provide shorthands for the next and previous execution dates, respectively [template_list](https://airflow.apache.org/docs/apache-airflow/stable/macros-ref.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-hardwood",
   "metadata": {},
   "source": [
    "#### Partitioning your data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-cincinnati",
   "metadata": {},
   "source": [
    "To divide our dataset into daily batches by writing the output of the task to a file bearing the name of the corresponding execution date:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-painting",
   "metadata": {},
   "source": [
    "```python\n",
    "fetch_events = BashOperator( task_id=\"fetch_events\", \n",
    "bash_command=(\n",
    "\"mkdir -p /data/events && \"\n",
    "\"curl -o /data/events/{{ds}}.json \" #A \"http://localhost:5000/events?\" \"start_date={{ds}}&\" \"end_date={{next_ds}}\",\n",
    "dag=dag, )                         \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-frame",
   "metadata": {},
   "source": [
    "Then with this partition approach we must change our stats_calc() function to use time templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-galaxy",
   "metadata": {},
   "source": [
    "```python\n",
    "def _calculate_stats(**context): #A\n",
    "    \n",
    "    \"\"\"Calculates event statistics.\"\"\"\n",
    "    input_path = context[\"templates_dict\"][\"input_path\"] #B \n",
    "    output_path = context[\"templates_dict\"][\"output_path\"]\n",
    "    Path(output_path).parent.mkdir(exist_ok=True)\n",
    "    events = pd.read_json(input_path)\n",
    "    \n",
    "    stats = events.groupby([\"date\", \"user\"]).size().reset_index() \n",
    "    stats.to_csv(output_path, index=False)\n",
    "    \n",
    "    calculate_stats = PythonOperator(task_id=\"calculate_stats\", \n",
    "                                     python_callable=_calculate_stats, \n",
    "                                     templates_dict={\n",
    "                                                    \"input_path\": \"/data/events/{{ds}}.json\", #C\n",
    "                                                    \"output_path\": \"/data/stats/{{ds}}.csv\", },\n",
    "    dag=dag, )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-marshall",
   "metadata": {},
   "source": [
    "- #A Receive all context variables in this dict.\n",
    "- #B Retrieve the templated values from the templates_dict object. \n",
    "- #C Pass the values that we want to be templated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strong-wrist",
   "metadata": {},
   "source": [
    "For Airflow 1.10.x, you’ll need to pass the extra argument provide_context=True to the PythonOperator, otherwise the _calculate_stats function won’t receive the context values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-prize",
   "metadata": {},
   "source": [
    "#### Understanding Airflow’s execution dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-dating",
   "metadata": {},
   "source": [
    "\n",
    "With Airflow execution dates being defined as the start of the corresponding schedule intervals, they can be used to derive the start and end of a specific interval (Figure 3.7). For example, when executing a task, the start and end of the corresponding interval are defined by the execution_date (the start of the interval) and the next_execution date (the start of the next interval) parameters. Similarly, the previous schedule interval can be derived using the previous_execution_date and execution_date parameters.\n",
    "However, one caveat to keep in mind when using the previous_execution_date and next_execution_date parameters in your tasks is that these parameters are only defined for DAG runs following the schedule interval. As such, the values of these parameters will be undefined for any runs that are triggered manually using Airflow UI or CLI. The reason for this is that Airflow cannot provide you with information about next or previous schedule intervals if you are not following a schedule interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-tucson",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH03_extraDef.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-saskatchewan",
   "metadata": {},
   "source": [
    "By default, Airflow will schedule and run any past schedule intervals that have not yet been run. As such, specifying a past start date and activating the corresponding DAG will result in all intervals that have passed before the current time being executed. This behavior is controlled by the DAG catchup parameter and can be disabled by setting catchup to False:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-somerset",
   "metadata": {},
   "source": [
    "```python\n",
    "dag = DAG(\n",
    "        dag_id=\"09_no_catchup\", \n",
    "        schedule_interval=\"@daily\", \n",
    "        start_date=dt.datetime(year=2019, month=1, day=1), \n",
    "        end_date=dt.datetime(year=2019, month=1, day=5), \n",
    "        catchup=False,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-representative",
   "metadata": {},
   "source": [
    "With this setting, the DAG will only be run for the most recent schedule interval, rather than executing all open past intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-ceremony",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH03catchup.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-cutting",
   "metadata": {},
   "source": [
    "Backfilling can also be used to re-process data after we have made changes in our code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-trouble",
   "metadata": {},
   "source": [
    "#### Best Practices for Designing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-worship",
   "metadata": {},
   "source": [
    "The term atomicity is frequently used in database systems, where an atomic transaction is considered to be an indivisible and irreducible series of database operations such that either all occur, or nothing occurs. Similarly, in Airflow, tasks should be defined so that they either succeed and produce some proper result, or fail in a manner that does not affect the state of the system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-concrete",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH03atomicity.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cordless-mainland",
   "metadata": {},
   "source": [
    "#### atomicity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-athletics",
   "metadata": {},
   "source": [
    "The atomicity is like the database property! for example evaluate statistics and send an email two tasks wich should be atomic, with the code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-thailand",
   "metadata": {},
   "source": [
    "```python\n",
    "def _send_stats(email, **context):\n",
    "    \n",
    "    stats = pd.read_csv(context[\"templates_dict\"][\"stats_path\"]) \n",
    "    email_stats(stats, email=email) #A\n",
    "    \n",
    "    send_stats = PythonOperator(\n",
    "                                task_id=\"send_stats\",\n",
    "                                python_callable=_send_stats,\n",
    "                                op_kwargs={\"email\": \"user@example.com\"}, \n",
    "                                templates_dict={\"stats_path\": \"/data/stats/{{ds}}.csv\"}, \n",
    "                                dag=dag,\n",
    ")\n",
    "    \n",
    "calculate_stats >> send_stats\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-surveillance",
   "metadata": {},
   "source": [
    "- #A Split off the email_stats statement into a separate task for atomicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-training",
   "metadata": {},
   "source": [
    "From this example, you might think that separating all operations into individual tasks is\n",
    "sufficient to make all our tasks atomic. This is however not necessarily true. To see why, think about what would if our event API would require us to log in before querying for events. This would generally require an extra API call to fetch some authentication token, after which we can start retrieving our events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-bathroom",
   "metadata": {},
   "source": [
    "#### Idempotency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-interest",
   "metadata": {},
   "source": [
    "Tasks are said to be idempotent if calling the same task multiple times with the same inputs has no additional effect. This means, for example, that re-running a task without changing the inputs should not change the overall output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-kuwait",
   "metadata": {},
   "source": [
    "<img src=\"./pic/CH03 Idempotency.png\" width=\"800\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-bradley",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-twist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-gravity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
