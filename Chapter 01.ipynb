{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "found-trunk",
   "metadata": {},
   "source": [
    "# Abstract of airflow chapter 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-buffer",
   "metadata": {},
   "source": [
    "1. Airflow uses DAGS\n",
    "2. It has an internal scheduler inside it\n",
    "3. The nodes are tasks to be executed, the dependency between task are the edges.\n",
    "4. Dags are writen in python with the tasks to be execute and some meta info about how to execute it inside airflow\n",
    "5. You could generate dinamic DAGS using another DAGS (warning this is never a good idea)\n",
    "6. Airflow has it own scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-engine",
   "metadata": {},
   "source": [
    "Running pipeline algorithm:\n",
    "\n",
    "- For each open (= uncompleted) task in the graph:\n",
    "- For each edge pointing towards the task, check if the ‘upstream’ task on the other end of the\n",
    "edge has been completed.\n",
    "- If all upstream tasks have been completed, add the task under consideration to a queue of\n",
    "tasks to be executed.\n",
    "- Execute the tasks in the execution queue, marking them completed once they finish\n",
    "performing their work.\n",
    "- Jump back to step 1, until all tasks in the graph have been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-huntington",
   "metadata": {},
   "source": [
    "<img src=\"./pic/scheduler_idea.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-report",
   "metadata": {},
   "source": [
    "Airflow is organized in three main components:\n",
    "\n",
    "1. Scheduler -> check all dags and execute then like crontab\n",
    "2. Workers -> pick up tasks scheduled and execute them\n",
    "3. Webserver -> GUI for monitoring the dags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-satin",
   "metadata": {},
   "source": [
    "<img src=\"./pic/ariflow_archtecture.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-covering",
   "metadata": {},
   "source": [
    "The heart of airflow is its scheduler, which runs in the following steps:\n",
    "\n",
    "- Once users have written their workflows as DAGs, the files containing these DAGs are read by the scheduler to extract the corresponding tasks, dependencies, and schedule interval of each DAG.\n",
    "- For each DAG, the scheduler then checks whether the schedule interval for the DAG has passed since the last time it was read. If so, the tasks in the DAG are scheduled for execution.\n",
    "- For each scheduled task, the scheduler then checks whether the dependencies (= upstream tasks) of the task have been completed. If so, the task is added to the execution queue.\n",
    "- The scheduler waits for several moments before starting a new loop by jumping back to step 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-banks",
   "metadata": {},
   "source": [
    "<img src=\"./pic/airflow.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-flesh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "military-fishing",
   "metadata": {},
   "source": [
    "Besides scheduling and executing DAGs, Airflow also provides an extensive web interface that can be used for viewing DAGs and for monitoring the results of DAG run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-community",
   "metadata": {},
   "source": [
    "<img src=\"./pic/webserver.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-bankruptcy",
   "metadata": {},
   "source": [
    "By default, Airflow can handle failures in tasks by retrying them a couple of times (optionally with some wait time in between), which can help tasks recover from any intermittent failures. If retries don’t help, Airflow will record the task as being failed, optionally notifying you about the failure if configured to do so. Debugging task failures is pretty straight forward, as the tree view allows you to view which tasks failed and dig into their logs. The same view also allows you to clear the results of individual tasks to re-run them (together with any tasks that depend on that task), allowing you to easily re-run any tasks after you make changes to their code\n",
    "\n",
    "One powerful feature of Airflow’s scheduling semantics is that the schedule intervals not only trigger DAGs at specific time points (similar to, for example, Cron) but also provide details about the last and (expected) next schedule intervals. Schedule intervals become even more powerful when combined with the concept of ‘backfilling’, which allows you to execute a new DAG for ‘historical’ schedule intervals that have already occurred in the past\n",
    "\n",
    "Features such as backfilling allow you to easily (re-)process historical data, allowing you to recompute any derived datasets after making changes to your code ( Eso es clave para SYNC de code y git y server \"test suit\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-turkish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-client",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
